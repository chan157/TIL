{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32, [None,784])\n",
    "y=tf.placeholder(tf.float32, [None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=tf.Variable(tf.random_normal([784,10]))\n",
    "b=tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.nn.softmax(tf.matmul(x,w)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(hf),axis=1))\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "isCorrect=tf.equal(tf.argmax(hf,1),tf.argmax(y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=tf.reduce_mean(tf.cast(isCorrect,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs=15\n",
    "batchSize=100\n",
    "numIter=int(mnist.train.num_examples/batchSize)\n",
    "# 60000 / 100 = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에폭:0001, cost:2.775853102\n",
      "에폭:0002, cost:1.124863169\n",
      "에폭:0003, cost:0.892708509\n",
      "에폭:0004, cost:0.779763279\n",
      "에폭:0005, cost:0.708565397\n",
      "에폭:0006, cost:0.658113061\n",
      "에폭:0007, cost:0.620275978\n",
      "에폭:0008, cost:0.589766805\n",
      "에폭:0009, cost:0.565169123\n",
      "에폭:0010, cost:0.544060498\n",
      "에폭:0011, cost:0.526480060\n",
      "에폭:0012, cost:0.510547252\n",
      "에폭:0013, cost:0.497044321\n",
      "에폭:0014, cost:0.484776964\n",
      "에폭:0015, cost:0.474462277\n",
      "정확도: 0.8901\n",
      "레이블: [5]\n",
      "예측: [5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #트레이닝\n",
    "    for epoch in range(numEpochs):#15에폭\n",
    "        avgCv=0\n",
    "        for i in range(numIter): #600\n",
    "            batchX,batchY=mnist.train.next_batch(batchSize)\n",
    "            _,cv=sess.run([train,cost], feed_dict={x:batchX,y:batchY})\n",
    "            avgCv+=cv/numIter\n",
    "        print(\"에폭:{:04d}, cost:{:.9f}\".format(epoch+1,avgCv))\n",
    "    print(\"정확도:\", accuracy.eval(session=sess, feed_dict={x:mnist.test.images, y:mnist.test.labels}))\n",
    "    r=random.randint(0,mnist.test.num_examples-1)\n",
    "    print(\"레이블:\",sess.run(tf.argmax(mnist.test.labels[r:r+1],1)))\n",
    "    print(\"예측:\", sess.run(tf.argmax(hf,1),feed_dict={x:mnist.test.images[r:r+1]}))\n",
    "    \n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28),\n",
    "              cmap='Greys')\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 모델 저장/불러오기 (keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#다층퍼셉트론 모델\n",
    "#훈련셋,검증셋,시험셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xTrain, yTrain), (xTest,yTest)\n",
    "=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리\n",
    "xTrain=xTrain.reshape(60000,784).astype('float32')/255.0\n",
    "xTest=xTest.reshape(10000,784).astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrain=np_utils.to_categorical(yTrain)\n",
    "yTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTest=np_utils.to_categorical(yTest)\n",
    "yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "xVal=xTrain[42000:]\n",
    "xTrain=xTrain[:42000]\n",
    "yVal=yTrain[42000:]\n",
    "yTrain=yTrain[:42000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#모델 구성\n",
    "model=Sequential()\n",
    "model.add(Dense(units=64, input_dim=28*28, activation=\"relu\"))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 1s 31us/step - loss: 0.9248 - accuracy: 0.7722 - val_loss: 0.5003 - val_accuracy: 0.8711\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.4479 - accuracy: 0.8807 - val_loss: 0.3898 - val_accuracy: 0.8926\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.3758 - accuracy: 0.8965 - val_loss: 0.3487 - val_accuracy: 0.9003\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 1s 26us/step - loss: 0.3414 - accuracy: 0.9045 - val_loss: 0.3255 - val_accuracy: 0.9068\n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.3190 - accuracy: 0.9095 - val_loss: 0.3093 - val_accuracy: 0.9111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x204dc394708>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습 환경 설정(compile)\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"sgd\",metrics=[\"accuracy\"])\n",
    "#학습(fit)\n",
    "model.fit(xTrain,yTrain,epochs=5, batch_size=50,validation_data=(xVal,yVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 13us/step\n",
      "평가결과:[0.29565307053737344, 0.9168000221252441]\n"
     ]
    }
   ],
   "source": [
    "#모델 평가하기(test data)\n",
    "metrics=model.evaluate(xTest,yTest,batch_size=50)\n",
    "print(\"평가결과:\"+str(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=np.random.choice(xTest.shape[0],5)\n",
    "xHat=xTest[idx]\n",
    "yHat=model.predict_classes(xHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값 : 6 실제값 : 6\n",
      "예측값 : 9 실제값 : 9\n",
      "예측값 : 8 실제값 : 8\n",
      "예측값 : 1 실제값 : 8\n",
      "예측값 : 0 실제값 : 0\n"
     ]
    }
   ],
   "source": [
    "#print(\"예측값:\",yHat) #예측값\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"예측값 : \"+ str(yHat[i]) + \" 실제값 : \"+str(np.argmax(yTest[idx[i]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "모델 : 모델 아키텍처와 모델 가중치로 구성\n",
    "모델 아키텍처:모델이 어떤 층으로 구성\n",
    "모델 가중치:weight, bias\n",
    "\n",
    "save():케라스 모델 저장 함수(아키텍처+가중치)\n",
    "파일형식 : \"h5\"로 저장\n",
    "\"\"\"\n",
    "model.save(\"mnist_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#모델 아키텍쳐 확인\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# from IPython.display import SVG\n",
    "# SVG(model_to_dot(model, show_shapes=True).\n",
    "#     create(prog=\"dot\", format=\"svg\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값 : 9 실제값 : 9\n",
      "예측값 : 4 실제값 : 4\n",
      "예측값 : 2 실제값 : 2\n",
      "예측값 : 5 실제값 : 5\n",
      "예측값 : 8 실제값 : 8\n",
      "예측값 : 3 실제값 : 3\n",
      "예측값 : 1 실제값 : 1\n",
      "예측값 : 5 실제값 : 5\n",
      "예측값 : 4 실제값 : 4\n",
      "예측값 : 9 실제값 : 8\n"
     ]
    }
   ],
   "source": [
    "#실제 데이터 사용\n",
    "(xTrain, yTrain), (xTest,yTest)=mnist.load_data()\n",
    "xTest=xTest.reshape(10000,784).astype('float32')/255.0\n",
    "yTest=np_utils.to_categorical(yTest)\n",
    "idx=np.random.choice(xTest.shape[0],10)\n",
    "xhat=xTest[idx]\n",
    "#모델 불러오기\n",
    "from keras.models import load_model\n",
    "model=load_model(\"mnist_model.h5\")\n",
    "yhat=model.predict_classes(xhat)\n",
    "for i in range(10):\n",
    "    print(\"예측값 : \"+ str(yhat[i]) + \n",
    "          \" 실제값 : \"+str(np.argmax(\n",
    "              yTest[idx[i]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata=xy[:,0:-1]\n",
    "ydata=xy[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,shape=[None,4])\n",
    "y=tf.placeholder(tf.float32,shape=[None,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=tf.Variable(tf.random_normal([4,1]))\n",
    "b=tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.matmul(x,w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(tf.square(hf-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(1e-5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 634701550000.0 \n",
      "Prediction: [[ -561878.9 ]\n",
      " [-1129612.8 ]\n",
      " [ -888932.9 ]\n",
      " [ -623567.8 ]\n",
      " [ -734653.3 ]\n",
      " [ -740826.1 ]\n",
      " [ -679100.56]\n",
      " [ -864235.6 ]]\n",
      "1 cost: 6.9733385e+26 \n",
      "Prediction: [[1.8627376e+13]\n",
      " [3.7498818e+13]\n",
      " [2.9498968e+13]\n",
      " [2.0678619e+13]\n",
      " [2.4370859e+13]\n",
      " [2.4575983e+13]\n",
      " [2.2524740e+13]\n",
      " [2.8678472e+13]]\n",
      "2 cost: inf \n",
      "Prediction: [[-6.1742958e+20]\n",
      " [-1.2429489e+21]\n",
      " [-9.7778313e+20]\n",
      " [-6.8542077e+20]\n",
      " [-8.0780504e+20]\n",
      " [-8.1460413e+20]\n",
      " [-7.4661294e+20]\n",
      " [-9.5058667e+20]]\n",
      "3 cost: inf \n",
      "Prediction: [[2.0465537e+28]\n",
      " [4.1199219e+28]\n",
      " [3.2409944e+28]\n",
      " [2.2719199e+28]\n",
      " [2.6775790e+28]\n",
      " [2.7001155e+28]\n",
      " [2.4747493e+28]\n",
      " [3.1508479e+28]]\n",
      "4 cost: inf \n",
      "Prediction: [[-6.7835787e+35]\n",
      " [-1.3656039e+36]\n",
      " [-1.0742714e+36]\n",
      " [-7.5305853e+35]\n",
      " [-8.8751974e+35]\n",
      " [-8.9498977e+35]\n",
      " [-8.2028910e+35]\n",
      " [-1.0443911e+36]]\n",
      "5 cost: inf \n",
      "Prediction: [[inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]]\n",
      "6 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "for step in range(101):\n",
    "    cv,hv,_=sess.run([cost,hf,train],\n",
    "                     feed_dict={x:xdata,y:ydata})\n",
    "    print(step,\"cost:\",cv,\"\\nPrediction:\",hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 5)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myMinMax(data):\n",
    "    #print(np.min(data)) #전체에서 최소값\n",
    "    #print(np.min(data, axis=1)) #행 단위로 최소값\n",
    "    #print(np.min(data, axis=0)) #열 단위로 최소값    \n",
    "    bj=data-np.min(data,0)\n",
    "    bm=np.max(data,0)-np.min(data,0)    \n",
    "    return bj/bm   \n",
    "    \n",
    "xy=myMinMax(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata=xy[:,0:-1]\n",
    "ydata=xy[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        ],\n",
       "       [0.83755792],\n",
       "       [0.6606331 ],\n",
       "       [0.43800918],\n",
       "       [0.42624401],\n",
       "       [0.49276137],\n",
       "       [0.18597238],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,shape=[None,4])\n",
    "y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "w=tf.Variable(tf.random_normal([4,1]))\n",
    "b=tf.Variable(tf.random_normal([1]))\n",
    "hf=tf.matmul(x,w)+b\n",
    "cost=tf.reduce_mean(tf.square(hf-y))\n",
    "train=tf.train.GradientDescentOptimizer(1e-5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 4.7671313 \n",
      "Prediction: [[-2.1291313]\n",
      " [-1.2809563]\n",
      " [-1.4712601]\n",
      " [-1.7489903]\n",
      " [-1.6053405]\n",
      " [-1.709004 ]\n",
      " [-1.5292794]\n",
      " [-1.6162782]]\n",
      "1 cost: 4.766775 \n",
      "Prediction: [[-2.1290216]\n",
      " [-1.2808509]\n",
      " [-1.4711716]\n",
      " [-1.7489209]\n",
      " [-1.6052588]\n",
      " [-1.7089251]\n",
      " [-1.5292256]\n",
      " [-1.6162256]]\n",
      "2 cost: 4.7664194 \n",
      "Prediction: [[-2.128912 ]\n",
      " [-1.2807453]\n",
      " [-1.4710829]\n",
      " [-1.7488514]\n",
      " [-1.6051772]\n",
      " [-1.7088461]\n",
      " [-1.5291718]\n",
      " [-1.616173 ]]\n",
      "3 cost: 4.7660637 \n",
      "Prediction: [[-2.1288023]\n",
      " [-1.2806399]\n",
      " [-1.4709944]\n",
      " [-1.7487819]\n",
      " [-1.6050954]\n",
      " [-1.7087673]\n",
      " [-1.5291181]\n",
      " [-1.6161206]]\n",
      "4 cost: 4.765707 \n",
      "Prediction: [[-2.1286926]\n",
      " [-1.2805343]\n",
      " [-1.4709058]\n",
      " [-1.7487124]\n",
      " [-1.6050138]\n",
      " [-1.7086883]\n",
      " [-1.5290643]\n",
      " [-1.616068 ]]\n",
      "5 cost: 4.765351 \n",
      "Prediction: [[-2.128583 ]\n",
      " [-1.2804289]\n",
      " [-1.4708172]\n",
      " [-1.7486429]\n",
      " [-1.6049322]\n",
      " [-1.7086093]\n",
      " [-1.5290105]\n",
      " [-1.6160154]]\n",
      "6 cost: 4.764995 \n",
      "Prediction: [[-2.1284733]\n",
      " [-1.2803233]\n",
      " [-1.4707286]\n",
      " [-1.7485735]\n",
      " [-1.6048504]\n",
      " [-1.7085304]\n",
      " [-1.5289568]\n",
      " [-1.615963 ]]\n",
      "7 cost: 4.7646394 \n",
      "Prediction: [[-2.1283636]\n",
      " [-1.2802178]\n",
      " [-1.4706401]\n",
      " [-1.7485039]\n",
      " [-1.6047688]\n",
      " [-1.7084515]\n",
      " [-1.528903 ]\n",
      " [-1.6159104]]\n",
      "8 cost: 4.764283 \n",
      "Prediction: [[-2.128254 ]\n",
      " [-1.2801123]\n",
      " [-1.4705515]\n",
      " [-1.7484345]\n",
      " [-1.6046871]\n",
      " [-1.7083725]\n",
      " [-1.5288494]\n",
      " [-1.6158578]]\n",
      "9 cost: 4.7639275 \n",
      "Prediction: [[-2.1281443]\n",
      " [-1.2800068]\n",
      " [-1.4704628]\n",
      " [-1.7483649]\n",
      " [-1.6046054]\n",
      " [-1.7082936]\n",
      " [-1.5287955]\n",
      " [-1.6158054]]\n",
      "10 cost: 4.763571 \n",
      "Prediction: [[-2.1280346]\n",
      " [-1.2799013]\n",
      " [-1.4703741]\n",
      " [-1.7482955]\n",
      " [-1.6045238]\n",
      " [-1.7082146]\n",
      " [-1.5287418]\n",
      " [-1.6157528]]\n",
      "11 cost: 4.7632155 \n",
      "Prediction: [[-2.1279252]\n",
      " [-1.2797958]\n",
      " [-1.4702857]\n",
      " [-1.748226 ]\n",
      " [-1.6044421]\n",
      " [-1.7081357]\n",
      " [-1.5286881]\n",
      " [-1.6157002]]\n",
      "12 cost: 4.7628593 \n",
      "Prediction: [[-2.1278155]\n",
      " [-1.2796903]\n",
      " [-1.470197 ]\n",
      " [-1.7481565]\n",
      " [-1.6043603]\n",
      " [-1.7080567]\n",
      " [-1.5286343]\n",
      " [-1.6156478]]\n",
      "13 cost: 4.7625036 \n",
      "Prediction: [[-2.1277058]\n",
      " [-1.2795846]\n",
      " [-1.4701084]\n",
      " [-1.748087 ]\n",
      " [-1.6042787]\n",
      " [-1.7079778]\n",
      " [-1.5285805]\n",
      " [-1.6155952]]\n",
      "14 cost: 4.762148 \n",
      "Prediction: [[-2.1275964]\n",
      " [-1.2794793]\n",
      " [-1.4700198]\n",
      " [-1.7480175]\n",
      " [-1.604197 ]\n",
      " [-1.7078989]\n",
      " [-1.5285268]\n",
      " [-1.6155427]]\n",
      "15 cost: 4.761792 \n",
      "Prediction: [[-2.1274867]\n",
      " [-1.2793736]\n",
      " [-1.4699314]\n",
      " [-1.7479482]\n",
      " [-1.6041154]\n",
      " [-1.7078199]\n",
      " [-1.528473 ]\n",
      " [-1.6154902]]\n",
      "16 cost: 4.7614365 \n",
      "Prediction: [[-2.127377 ]\n",
      " [-1.2792683]\n",
      " [-1.4698428]\n",
      " [-1.7478787]\n",
      " [-1.6040337]\n",
      " [-1.707741 ]\n",
      " [-1.5284193]\n",
      " [-1.6154376]]\n",
      "17 cost: 4.7610803 \n",
      "Prediction: [[-2.1272674]\n",
      " [-1.2791626]\n",
      " [-1.4697542]\n",
      " [-1.7478092]\n",
      " [-1.603952 ]\n",
      " [-1.7076621]\n",
      " [-1.5283656]\n",
      " [-1.615385 ]]\n",
      "18 cost: 4.7607245 \n",
      "Prediction: [[-2.1271577]\n",
      " [-1.2790573]\n",
      " [-1.4696655]\n",
      " [-1.7477397]\n",
      " [-1.6038703]\n",
      " [-1.7075831]\n",
      " [-1.5283117]\n",
      " [-1.6153326]]\n",
      "19 cost: 4.7603683 \n",
      "Prediction: [[-2.127048 ]\n",
      " [-1.2789518]\n",
      " [-1.469577 ]\n",
      " [-1.7476702]\n",
      " [-1.6037886]\n",
      " [-1.7075042]\n",
      " [-1.5282581]\n",
      " [-1.61528  ]]\n",
      "20 cost: 4.7600126 \n",
      "Prediction: [[-2.1269383]\n",
      " [-1.2788463]\n",
      " [-1.4694884]\n",
      " [-1.7476007]\n",
      " [-1.6037071]\n",
      " [-1.7074252]\n",
      " [-1.5282042]\n",
      " [-1.6152275]]\n",
      "21 cost: 4.759657 \n",
      "Prediction: [[-2.1268287]\n",
      " [-1.2787406]\n",
      " [-1.4693998]\n",
      " [-1.7475312]\n",
      " [-1.6036253]\n",
      " [-1.7073463]\n",
      " [-1.5281506]\n",
      " [-1.615175 ]]\n",
      "22 cost: 4.759302 \n",
      "Prediction: [[-2.1267192]\n",
      " [-1.2786353]\n",
      " [-1.4693112]\n",
      " [-1.7474618]\n",
      " [-1.6035438]\n",
      " [-1.7072674]\n",
      " [-1.5280968]\n",
      " [-1.6151226]]\n",
      "23 cost: 4.7589455 \n",
      "Prediction: [[-2.1266096]\n",
      " [-1.2785298]\n",
      " [-1.4692225]\n",
      " [-1.7473923]\n",
      " [-1.603462 ]\n",
      " [-1.7071885]\n",
      " [-1.528043 ]\n",
      " [-1.6150699]]\n",
      "24 cost: 4.7585897 \n",
      "Prediction: [[-2.1265001]\n",
      " [-1.2784243]\n",
      " [-1.469134 ]\n",
      " [-1.7473228]\n",
      " [-1.6033802]\n",
      " [-1.7071095]\n",
      " [-1.5279893]\n",
      " [-1.6150174]]\n",
      "25 cost: 4.758234 \n",
      "Prediction: [[-2.1263905]\n",
      " [-1.2783188]\n",
      " [-1.4690454]\n",
      " [-1.7472533]\n",
      " [-1.6032987]\n",
      " [-1.7070305]\n",
      " [-1.5279355]\n",
      " [-1.614965 ]]\n",
      "26 cost: 4.757879 \n",
      "Prediction: [[-2.1262808]\n",
      " [-1.2782133]\n",
      " [-1.4689568]\n",
      " [-1.7471838]\n",
      " [-1.6032169]\n",
      " [-1.7069517]\n",
      " [-1.5278817]\n",
      " [-1.6149124]]\n",
      "27 cost: 4.7575226 \n",
      "Prediction: [[-2.126171 ]\n",
      " [-1.2781079]\n",
      " [-1.4688683]\n",
      " [-1.7471144]\n",
      " [-1.6031353]\n",
      " [-1.7068727]\n",
      " [-1.527828 ]\n",
      " [-1.6148598]]\n",
      "28 cost: 4.757167 \n",
      "Prediction: [[-2.1260614]\n",
      " [-1.2780023]\n",
      " [-1.4687797]\n",
      " [-1.7470449]\n",
      " [-1.6030537]\n",
      " [-1.7067938]\n",
      " [-1.5277743]\n",
      " [-1.6148072]]\n",
      "29 cost: 4.756811 \n",
      "Prediction: [[-2.1259518]\n",
      " [-1.2778968]\n",
      " [-1.4686911]\n",
      " [-1.7469754]\n",
      " [-1.6029719]\n",
      " [-1.7067149]\n",
      " [-1.5277205]\n",
      " [-1.6147548]]\n",
      "30 cost: 4.756456 \n",
      "Prediction: [[-2.125842 ]\n",
      " [-1.2777913]\n",
      " [-1.4686025]\n",
      " [-1.7469059]\n",
      " [-1.6028903]\n",
      " [-1.706636 ]\n",
      " [-1.5276668]\n",
      " [-1.6147022]]\n",
      "31 cost: 4.7561007 \n",
      "Prediction: [[-2.1257327]\n",
      " [-1.2776859]\n",
      " [-1.468514 ]\n",
      " [-1.7468364]\n",
      " [-1.6028087]\n",
      " [-1.706557 ]\n",
      " [-1.527613 ]\n",
      " [-1.6146497]]\n",
      "32 cost: 4.7557445 \n",
      "Prediction: [[-2.125623 ]\n",
      " [-1.2775803]\n",
      " [-1.4684254]\n",
      " [-1.7467669]\n",
      " [-1.6027269]\n",
      " [-1.7064781]\n",
      " [-1.5275593]\n",
      " [-1.6145972]]\n",
      "33 cost: 4.755389 \n",
      "Prediction: [[-2.1255136]\n",
      " [-1.2774749]\n",
      " [-1.4683368]\n",
      " [-1.7466975]\n",
      " [-1.6026453]\n",
      " [-1.7063991]\n",
      " [-1.5275055]\n",
      " [-1.6145446]]\n",
      "34 cost: 4.7550335 \n",
      "Prediction: [[-2.125404 ]\n",
      " [-1.2773693]\n",
      " [-1.4682481]\n",
      " [-1.746628 ]\n",
      " [-1.6025636]\n",
      " [-1.7063203]\n",
      " [-1.5274518]\n",
      " [-1.614492 ]]\n",
      "35 cost: 4.7546782 \n",
      "Prediction: [[-2.1252942]\n",
      " [-1.2772639]\n",
      " [-1.4681596]\n",
      " [-1.7465585]\n",
      " [-1.602482 ]\n",
      " [-1.7062413]\n",
      " [-1.527398 ]\n",
      " [-1.6144396]]\n",
      "36 cost: 4.754322 \n",
      "Prediction: [[-2.1251845]\n",
      " [-1.2771583]\n",
      " [-1.468071 ]\n",
      " [-1.746489 ]\n",
      " [-1.6024003]\n",
      " [-1.7061623]\n",
      " [-1.5273442]\n",
      " [-1.614387 ]]\n",
      "37 cost: 4.753967 \n",
      "Prediction: [[-2.1250749]\n",
      " [-1.2770529]\n",
      " [-1.4679824]\n",
      " [-1.7464197]\n",
      " [-1.6023186]\n",
      " [-1.7060834]\n",
      " [-1.5272906]\n",
      " [-1.6143346]]\n",
      "38 cost: 4.7536116 \n",
      "Prediction: [[-2.1249652]\n",
      " [-1.2769473]\n",
      " [-1.467894 ]\n",
      " [-1.7463502]\n",
      " [-1.602237 ]\n",
      " [-1.7060045]\n",
      " [-1.5272368]\n",
      " [-1.614282 ]]\n",
      "39 cost: 4.7532563 \n",
      "Prediction: [[-2.1248558]\n",
      " [-1.2768419]\n",
      " [-1.4678054]\n",
      " [-1.7462807]\n",
      " [-1.6021552]\n",
      " [-1.7059255]\n",
      " [-1.527183 ]\n",
      " [-1.6142294]]\n",
      "40 cost: 4.7529006 \n",
      "Prediction: [[-2.124746 ]\n",
      " [-1.2767364]\n",
      " [-1.4677167]\n",
      " [-1.7462112]\n",
      " [-1.6020737]\n",
      " [-1.7058467]\n",
      " [-1.5271293]\n",
      " [-1.614177 ]]\n",
      "41 cost: 4.7525454 \n",
      "Prediction: [[-2.1246367]\n",
      " [-1.2766309]\n",
      " [-1.4676282]\n",
      " [-1.7461417]\n",
      " [-1.601992 ]\n",
      " [-1.7057676]\n",
      " [-1.5270755]\n",
      " [-1.6141244]]\n",
      "42 cost: 4.7521896 \n",
      "Prediction: [[-2.124527 ]\n",
      " [-1.2765255]\n",
      " [-1.4675395]\n",
      " [-1.7460723]\n",
      " [-1.6019104]\n",
      " [-1.7056887]\n",
      " [-1.5270218]\n",
      " [-1.6140718]]\n",
      "43 cost: 4.7518344 \n",
      "Prediction: [[-2.1244173]\n",
      " [-1.2764199]\n",
      " [-1.467451 ]\n",
      " [-1.7460028]\n",
      " [-1.6018287]\n",
      " [-1.7056098]\n",
      " [-1.526968 ]\n",
      " [-1.6140194]]\n",
      "44 cost: 4.751479 \n",
      "Prediction: [[-2.1243076]\n",
      " [-1.2763144]\n",
      " [-1.4673624]\n",
      " [-1.7459333]\n",
      " [-1.601747 ]\n",
      " [-1.7055309]\n",
      " [-1.5269144]\n",
      " [-1.6139668]]\n",
      "45 cost: 4.751124 \n",
      "Prediction: [[-2.1241982]\n",
      " [-1.2762091]\n",
      " [-1.467274 ]\n",
      " [-1.7458639]\n",
      " [-1.6016654]\n",
      " [-1.705452 ]\n",
      " [-1.5268605]\n",
      " [-1.6139143]]\n",
      "46 cost: 4.7507687 \n",
      "Prediction: [[-2.1240888]\n",
      " [-1.2761036]\n",
      " [-1.4671855]\n",
      " [-1.7457944]\n",
      " [-1.6015837]\n",
      " [-1.7053732]\n",
      " [-1.5268068]\n",
      " [-1.6138618]]\n",
      "47 cost: 4.7504134 \n",
      "Prediction: [[-2.123979 ]\n",
      " [-1.2759982]\n",
      " [-1.4670969]\n",
      " [-1.745725 ]\n",
      " [-1.6015022]\n",
      " [-1.7052944]\n",
      " [-1.5267532]\n",
      " [-1.6138092]]\n",
      "48 cost: 4.750059 \n",
      "Prediction: [[-2.12387  ]\n",
      " [-1.2758929]\n",
      " [-1.4670085]\n",
      " [-1.7456557]\n",
      " [-1.6014206]\n",
      " [-1.7052155]\n",
      " [-1.5266994]\n",
      " [-1.6137567]]\n",
      "49 cost: 4.7497034 \n",
      "Prediction: [[-2.1237602]\n",
      " [-1.2757875]\n",
      " [-1.4669199]\n",
      " [-1.7455862]\n",
      " [-1.601339 ]\n",
      " [-1.7051365]\n",
      " [-1.5266457]\n",
      " [-1.6137042]]\n",
      "50 cost: 4.7493486 \n",
      "Prediction: [[-2.1236508]\n",
      " [-1.2756821]\n",
      " [-1.4668314]\n",
      " [-1.7455168]\n",
      " [-1.6012573]\n",
      " [-1.7050577]\n",
      " [-1.526592 ]\n",
      " [-1.6136516]]\n",
      "51 cost: 4.7489934 \n",
      "Prediction: [[-2.1235414]\n",
      " [-1.2755767]\n",
      " [-1.4667429]\n",
      " [-1.7454474]\n",
      " [-1.6011758]\n",
      " [-1.7049788]\n",
      " [-1.5265383]\n",
      " [-1.6135991]]\n",
      "52 cost: 4.748638 \n",
      "Prediction: [[-2.1234317]\n",
      " [-1.2754712]\n",
      " [-1.4666545]\n",
      " [-1.745378 ]\n",
      " [-1.6010942]\n",
      " [-1.7049   ]\n",
      " [-1.5264845]\n",
      " [-1.6135466]]\n",
      "53 cost: 4.7482834 \n",
      "Prediction: [[-2.1233225]\n",
      " [-1.2753658]\n",
      " [-1.466566 ]\n",
      " [-1.7453086]\n",
      " [-1.6010127]\n",
      " [-1.7048211]\n",
      " [-1.5264308]\n",
      " [-1.613494 ]]\n",
      "54 cost: 4.747928 \n",
      "Prediction: [[-2.1232128]\n",
      " [-1.2752604]\n",
      " [-1.4664774]\n",
      " [-1.7452391]\n",
      " [-1.6009312]\n",
      " [-1.7047422]\n",
      " [-1.5263771]\n",
      " [-1.6134415]]\n",
      "55 cost: 4.7475734 \n",
      "Prediction: [[-2.1231034]\n",
      " [-1.2751551]\n",
      " [-1.466389 ]\n",
      " [-1.7451698]\n",
      " [-1.6008495]\n",
      " [-1.7046633]\n",
      " [-1.5263233]\n",
      " [-1.613389 ]]\n",
      "56 cost: 4.7472186 \n",
      "Prediction: [[-2.122994 ]\n",
      " [-1.2750497]\n",
      " [-1.4663005]\n",
      " [-1.7451003]\n",
      " [-1.6007679]\n",
      " [-1.7045845]\n",
      " [-1.5262697]\n",
      " [-1.6133364]]\n",
      "57 cost: 4.7468634 \n",
      "Prediction: [[-2.1228843]\n",
      " [-1.2749443]\n",
      " [-1.4662119]\n",
      " [-1.7450309]\n",
      " [-1.6006863]\n",
      " [-1.7045057]\n",
      " [-1.5262159]\n",
      " [-1.6132839]]\n",
      "58 cost: 4.746509 \n",
      "Prediction: [[-2.122775 ]\n",
      " [-1.2748389]\n",
      " [-1.4661233]\n",
      " [-1.7449615]\n",
      " [-1.6006048]\n",
      " [-1.7044268]\n",
      " [-1.5261621]\n",
      " [-1.6132314]]\n",
      "59 cost: 4.7461543 \n",
      "Prediction: [[-2.1226656]\n",
      " [-1.2747337]\n",
      " [-1.466035 ]\n",
      " [-1.7448922]\n",
      " [-1.6005232]\n",
      " [-1.7043481]\n",
      " [-1.5261085]\n",
      " [-1.613179 ]]\n",
      "60 cost: 4.7458 \n",
      "Prediction: [[-2.1225562]\n",
      " [-1.2746284]\n",
      " [-1.4659467]\n",
      " [-1.7448229]\n",
      " [-1.6004417]\n",
      " [-1.7042693]\n",
      " [-1.526055 ]\n",
      " [-1.6131265]]\n",
      "61 cost: 4.745446 \n",
      "Prediction: [[-2.122447 ]\n",
      " [-1.274523 ]\n",
      " [-1.4658583]\n",
      " [-1.7447536]\n",
      " [-1.6003604]\n",
      " [-1.7041905]\n",
      " [-1.5260015]\n",
      " [-1.6130742]]\n",
      "62 cost: 4.7450914 \n",
      "Prediction: [[-2.1223376]\n",
      " [-1.2744179]\n",
      " [-1.46577  ]\n",
      " [-1.7446843]\n",
      " [-1.6002789]\n",
      " [-1.7041118]\n",
      " [-1.5259478]\n",
      " [-1.6130217]]\n",
      "63 cost: 4.7447367 \n",
      "Prediction: [[-2.1222281]\n",
      " [-1.2743125]\n",
      " [-1.4656816]\n",
      " [-1.7446151]\n",
      " [-1.6001974]\n",
      " [-1.7040331]\n",
      " [-1.5258942]\n",
      " [-1.6129694]]\n",
      "64 cost: 4.7443833 \n",
      "Prediction: [[-2.122119 ]\n",
      " [-1.2742074]\n",
      " [-1.4655932]\n",
      " [-1.7445457]\n",
      " [-1.600116 ]\n",
      " [-1.7039543]\n",
      " [-1.5258405]\n",
      " [-1.612917 ]]\n",
      "65 cost: 4.744028 \n",
      "Prediction: [[-2.1220095]\n",
      " [-1.274102 ]\n",
      " [-1.4655049]\n",
      " [-1.7444764]\n",
      " [-1.6000345]\n",
      " [-1.7038755]\n",
      " [-1.525787 ]\n",
      " [-1.6128645]]\n",
      "66 cost: 4.7436743 \n",
      "Prediction: [[-2.1219   ]\n",
      " [-1.2739968]\n",
      " [-1.4654164]\n",
      " [-1.7444072]\n",
      " [-1.599953 ]\n",
      " [-1.7037969]\n",
      " [-1.5257334]\n",
      " [-1.612812 ]]\n",
      "67 cost: 4.7433205 \n",
      "Prediction: [[-2.121791 ]\n",
      " [-1.2738914]\n",
      " [-1.4653281]\n",
      " [-1.7443378]\n",
      " [-1.5998716]\n",
      " [-1.7037182]\n",
      " [-1.5256798]\n",
      " [-1.6127597]]\n",
      "68 cost: 4.7429657 \n",
      "Prediction: [[-2.1216815]\n",
      " [-1.2737863]\n",
      " [-1.4652398]\n",
      " [-1.7442685]\n",
      " [-1.5997901]\n",
      " [-1.7036394]\n",
      " [-1.5256262]\n",
      " [-1.6127073]]\n",
      "69 cost: 4.742612 \n",
      "Prediction: [[-2.121572 ]\n",
      " [-1.2736809]\n",
      " [-1.4651513]\n",
      " [-1.7441993]\n",
      " [-1.5997087]\n",
      " [-1.7035607]\n",
      " [-1.5255725]\n",
      " [-1.6126549]]\n",
      "70 cost: 4.742257 \n",
      "Prediction: [[-2.1214628]\n",
      " [-1.2735758]\n",
      " [-1.465063 ]\n",
      " [-1.7441299]\n",
      " [-1.5996273]\n",
      " [-1.7034819]\n",
      " [-1.5255189]\n",
      " [-1.6126025]]\n",
      "71 cost: 4.7419033 \n",
      "Prediction: [[-2.1213534]\n",
      " [-1.2734704]\n",
      " [-1.4649746]\n",
      " [-1.7440608]\n",
      " [-1.5995457]\n",
      " [-1.7034032]\n",
      " [-1.5254654]\n",
      " [-1.61255  ]]\n",
      "72 cost: 4.7415485 \n",
      "Prediction: [[-2.121244 ]\n",
      " [-1.2733653]\n",
      " [-1.4648862]\n",
      " [-1.7439914]\n",
      " [-1.5994642]\n",
      " [-1.7033244]\n",
      " [-1.5254117]\n",
      " [-1.6124976]]\n",
      "73 cost: 4.7411947 \n",
      "Prediction: [[-2.1211348]\n",
      " [-1.2732599]\n",
      " [-1.4647979]\n",
      " [-1.7439221]\n",
      " [-1.5993829]\n",
      " [-1.7032458]\n",
      " [-1.5253582]\n",
      " [-1.6124452]]\n",
      "74 cost: 4.740841 \n",
      "Prediction: [[-2.1210256]\n",
      " [-1.2731547]\n",
      " [-1.4647095]\n",
      " [-1.7438529]\n",
      " [-1.5993013]\n",
      " [-1.703167 ]\n",
      " [-1.5253046]\n",
      " [-1.6123928]]\n",
      "75 cost: 4.740486 \n",
      "Prediction: [[-2.120916 ]\n",
      " [-1.2730494]\n",
      " [-1.4646211]\n",
      " [-1.7437835]\n",
      " [-1.5992199]\n",
      " [-1.7030882]\n",
      " [-1.5252509]\n",
      " [-1.6123405]]\n",
      "76 cost: 4.7401323 \n",
      "Prediction: [[-2.1208067]\n",
      " [-1.2729441]\n",
      " [-1.4645327]\n",
      " [-1.7437142]\n",
      " [-1.5991384]\n",
      " [-1.7030095]\n",
      " [-1.5251974]\n",
      " [-1.612288 ]]\n",
      "77 cost: 4.7397785 \n",
      "Prediction: [[-2.1206975]\n",
      " [-1.2728388]\n",
      " [-1.4644444]\n",
      " [-1.743645 ]\n",
      " [-1.599057 ]\n",
      " [-1.7029307]\n",
      " [-1.5251437]\n",
      " [-1.6122355]]\n",
      "78 cost: 4.739424 \n",
      "Prediction: [[-2.120588 ]\n",
      " [-1.2727336]\n",
      " [-1.464356 ]\n",
      " [-1.7435757]\n",
      " [-1.5989755]\n",
      " [-1.702852 ]\n",
      " [-1.5250902]\n",
      " [-1.6121832]]\n",
      "79 cost: 4.73907 \n",
      "Prediction: [[-2.1204786]\n",
      " [-1.2726283]\n",
      " [-1.4642676]\n",
      " [-1.7435064]\n",
      " [-1.5988941]\n",
      " [-1.7027732]\n",
      " [-1.5250366]\n",
      " [-1.6121308]]\n",
      "80 cost: 4.738716 \n",
      "Prediction: [[-2.1203694]\n",
      " [-1.2725232]\n",
      " [-1.4641793]\n",
      " [-1.743437 ]\n",
      " [-1.5988126]\n",
      " [-1.7026947]\n",
      " [-1.5249829]\n",
      " [-1.6120783]]\n",
      "81 cost: 4.738362 \n",
      "Prediction: [[-2.12026  ]\n",
      " [-1.2724178]\n",
      " [-1.4640908]\n",
      " [-1.7433678]\n",
      " [-1.598731 ]\n",
      " [-1.7026157]\n",
      " [-1.5249293]\n",
      " [-1.612026 ]]\n",
      "82 cost: 4.7380075 \n",
      "Prediction: [[-2.1201506]\n",
      " [-1.2723126]\n",
      " [-1.4640026]\n",
      " [-1.7432985]\n",
      " [-1.5986497]\n",
      " [-1.7025371]\n",
      " [-1.5248758]\n",
      " [-1.6119735]]\n",
      "83 cost: 4.7376537 \n",
      "Prediction: [[-2.1200414]\n",
      " [-1.2722075]\n",
      " [-1.4639142]\n",
      " [-1.7432293]\n",
      " [-1.5985682]\n",
      " [-1.7024584]\n",
      " [-1.5248222]\n",
      " [-1.6119212]]\n",
      "84 cost: 4.7373004 \n",
      "Prediction: [[-2.1199322]\n",
      " [-1.2721022]\n",
      " [-1.463826 ]\n",
      " [-1.74316  ]\n",
      " [-1.5984869]\n",
      " [-1.7023797]\n",
      " [-1.5247686]\n",
      " [-1.6118689]]\n",
      "85 cost: 4.736946 \n",
      "Prediction: [[-2.1198227]\n",
      " [-1.271997 ]\n",
      " [-1.4637376]\n",
      " [-1.7430906]\n",
      " [-1.5984054]\n",
      " [-1.702301 ]\n",
      " [-1.524715 ]\n",
      " [-1.6118164]]\n",
      "86 cost: 4.7365923 \n",
      "Prediction: [[-2.1197133]\n",
      " [-1.2718918]\n",
      " [-1.4636493]\n",
      " [-1.7430214]\n",
      " [-1.5983241]\n",
      " [-1.7022223]\n",
      " [-1.5246614]\n",
      " [-1.611764 ]]\n",
      "87 cost: 4.7362385 \n",
      "Prediction: [[-2.119604 ]\n",
      " [-1.2717867]\n",
      " [-1.4635609]\n",
      " [-1.7429521]\n",
      " [-1.5982425]\n",
      " [-1.7021434]\n",
      " [-1.5246079]\n",
      " [-1.6117117]]\n",
      "88 cost: 4.7358847 \n",
      "Prediction: [[-2.1194947]\n",
      " [-1.2716815]\n",
      " [-1.4634726]\n",
      " [-1.7428828]\n",
      " [-1.5981611]\n",
      " [-1.7020649]\n",
      " [-1.5245543]\n",
      " [-1.6116593]]\n",
      "89 cost: 4.735531 \n",
      "Prediction: [[-2.1193852]\n",
      " [-1.2715764]\n",
      " [-1.4633844]\n",
      " [-1.7428136]\n",
      " [-1.5980797]\n",
      " [-1.7019861]\n",
      " [-1.5245006]\n",
      " [-1.611607 ]]\n",
      "90 cost: 4.735177 \n",
      "Prediction: [[-2.119276 ]\n",
      " [-1.271471 ]\n",
      " [-1.4632959]\n",
      " [-1.7427442]\n",
      " [-1.5979983]\n",
      " [-1.7019074]\n",
      " [-1.5244471]\n",
      " [-1.6115545]]\n",
      "91 cost: 4.7348237 \n",
      "Prediction: [[-2.1191669]\n",
      " [-1.2713659]\n",
      " [-1.4632077]\n",
      " [-1.742675 ]\n",
      " [-1.5979168]\n",
      " [-1.7018287]\n",
      " [-1.5243936]\n",
      " [-1.6115022]]\n",
      "92 cost: 4.7344694 \n",
      "Prediction: [[-2.1190574]\n",
      " [-1.2712607]\n",
      " [-1.4631193]\n",
      " [-1.7426057]\n",
      " [-1.5978353]\n",
      " [-1.70175  ]\n",
      " [-1.5243399]\n",
      " [-1.6114497]]\n",
      "93 cost: 4.7341156 \n",
      "Prediction: [[-2.118948 ]\n",
      " [-1.2711555]\n",
      " [-1.4630309]\n",
      " [-1.7425364]\n",
      " [-1.597754 ]\n",
      " [-1.7016712]\n",
      " [-1.5242863]\n",
      " [-1.6113974]]\n",
      "94 cost: 4.733762 \n",
      "Prediction: [[-2.1188388]\n",
      " [-1.2710503]\n",
      " [-1.4629426]\n",
      " [-1.7424672]\n",
      " [-1.5976725]\n",
      " [-1.7015926]\n",
      " [-1.5242327]\n",
      " [-1.611345 ]]\n",
      "95 cost: 4.733408 \n",
      "Prediction: [[-2.1187296]\n",
      " [-1.2709451]\n",
      " [-1.4628543]\n",
      " [-1.7423979]\n",
      " [-1.597591 ]\n",
      " [-1.7015138]\n",
      " [-1.5241792]\n",
      " [-1.6112926]]\n",
      "96 cost: 4.733055 \n",
      "Prediction: [[-2.1186202]\n",
      " [-1.2708399]\n",
      " [-1.462766 ]\n",
      " [-1.7423286]\n",
      " [-1.5975096]\n",
      " [-1.7014351]\n",
      " [-1.5241256]\n",
      " [-1.6112403]]\n",
      "97 cost: 4.732701 \n",
      "Prediction: [[-2.1185107]\n",
      " [-1.2707348]\n",
      " [-1.4626777]\n",
      " [-1.7422594]\n",
      " [-1.5974281]\n",
      " [-1.7013564]\n",
      " [-1.5240719]\n",
      " [-1.6111879]]\n",
      "98 cost: 4.732347 \n",
      "Prediction: [[-2.1184015]\n",
      " [-1.2706295]\n",
      " [-1.4625894]\n",
      " [-1.7421901]\n",
      " [-1.5973468]\n",
      " [-1.7012777]\n",
      " [-1.5240184]\n",
      " [-1.6111355]]\n",
      "99 cost: 4.7319937 \n",
      "Prediction: [[-2.118292 ]\n",
      " [-1.2705243]\n",
      " [-1.462501 ]\n",
      " [-1.7421207]\n",
      " [-1.5972652]\n",
      " [-1.701199 ]\n",
      " [-1.5239648]\n",
      " [-1.611083 ]]\n",
      "100 cost: 4.73164 \n",
      "Prediction: [[-2.1181827]\n",
      " [-1.2704191]\n",
      " [-1.4624126]\n",
      " [-1.7420515]\n",
      " [-1.5971838]\n",
      " [-1.7011203]\n",
      " [-1.5239112]\n",
      " [-1.6110308]]\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(101):\n",
    "    cv,hv,_=sess.run([cost,hf,train],\n",
    "                     feed_dict={x:xdata,y:ydata})\n",
    "    print(step,\"cost:\",cv,\"\\nPrediction:\",hv)\n",
    "    \n",
    "    #1. 역정규화를 하여 예측 종가를 출력\n",
    "    #1.874 => xxxx???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xor 문제를 텐서플로우로 구현\n",
    "#단일, 멀티 퍼셉트론 각각 구현\n",
    "xdata=np.array([[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]])\n",
    "ydata=np.array([[0],\n",
    "               [1],\n",
    "               [1],\n",
    "               [0]])\n",
    "#트레이닝 횟수:10000번, lr=0.1\n",
    "# 예측값 출력 \n",
    "# 0 0 => 0\n",
    "# 0 1 => 1\n",
    "# 1 0 => 1\n",
    "# 1 1 => 0\n",
    "#정확도:0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6991447 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "100 0.6954743 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "200 0.6940905 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "300 0.69354343 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "400 0.69331807 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "500 0.6932223 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "600 0.6931806 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "700 0.69316214 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "800 0.693154 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "900 0.6931503 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "1000 0.6931486 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "1100 0.69314784 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "1200 0.6931475 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "1300 0.69314736 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "1400 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "1500 0.69314724 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "1600 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "1700 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "1800 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "1900 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "2000 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "2100 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "2200 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "2300 0.69314724 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "2400 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "2500 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "2600 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "2700 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "2800 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "2900 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "3000 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "3100 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "3200 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "3300 0.6931471 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "3400 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "3500 0.6931471 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "3600 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "3700 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "3800 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "3900 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "4000 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "4100 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "4200 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "4300 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "4400 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "4500 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "4600 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "4700 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "4800 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "4900 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "5000 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "5100 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "5200 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "5300 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "5400 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "5500 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "5600 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "5700 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "5800 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "5900 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "6000 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "6100 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "6200 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "6300 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "6400 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "6500 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "6600 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "6700 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "6800 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "6900 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "7000 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "7100 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "7200 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "7300 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "7400 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "7500 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "7600 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "7700 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "7800 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "7900 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "8000 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "8100 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "8200 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "8300 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "8400 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "8500 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "8600 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "8700 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "8800 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "8900 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "9000 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "9100 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "9200 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "9300 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "9400 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "9500 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "9600 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "9700 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "9800 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "9900 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "10000 0.6931472 [[ 0.47229525]\n",
      " [-0.84693116]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w = tf.Variable(tf.random_normal([2, 1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "hf= tf.sigmoid(tf.matmul(x, w) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hf) + (1 - y) * tf.log(1 - hf))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hf > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cv, wv = sess.run(\n",
    "                  [train, cost, w], feed_dict={x: xdata, y: ydata}\n",
    "        )\n",
    "        if step % 100 == 0:\n",
    "            print(step, cv, wv)\n",
    "\n",
    "    h, c, a = sess.run([hf, predicted, accuracy], feed_dict={x: xdata, y: ydata})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.95840865\n",
      "100 0.7303478\n",
      "200 0.7036754\n",
      "300 0.6915255\n",
      "400 0.68383193\n",
      "500 0.6776713\n",
      "600 0.67194307\n",
      "700 0.6661573\n",
      "800 0.66005766\n",
      "900 0.6534934\n",
      "1000 0.64638174\n",
      "1100 0.6387012\n",
      "1200 0.6304938\n",
      "1300 0.621861\n",
      "1400 0.61294925\n",
      "1500 0.6039276\n",
      "1600 0.5949622\n",
      "1700 0.58619547\n",
      "1800 0.5777346\n",
      "1900 0.56964767\n",
      "2000 0.5619674\n",
      "2100 0.5546973\n",
      "2200 0.54781973\n",
      "2300 0.541302\n",
      "2400 0.5351018\n",
      "2500 0.529171\n",
      "2600 0.52345884\n",
      "2700 0.5179142\n",
      "2800 0.51248896\n",
      "2900 0.50713974\n",
      "3000 0.50183177\n",
      "3100 0.49654043\n",
      "3200 0.49125287\n",
      "3300 0.48596847\n",
      "3400 0.4806974\n",
      "3500 0.47545865\n",
      "3600 0.47027695\n",
      "3700 0.46517968\n",
      "3800 0.46019408\n",
      "3900 0.4553448\n",
      "4000 0.450653\n",
      "4100 0.44613487\n",
      "4200 0.44180197\n",
      "4300 0.4376614\n",
      "4400 0.43371588\n",
      "4500 0.42996514\n",
      "4600 0.4264063\n",
      "4700 0.42303398\n",
      "4800 0.41984203\n",
      "4900 0.41682303\n",
      "5000 0.4139688\n",
      "5100 0.41127118\n",
      "5200 0.4087217\n",
      "5300 0.4063123\n",
      "5400 0.40403503\n",
      "5500 0.4018818\n",
      "5600 0.3998454\n",
      "5700 0.3979187\n",
      "5800 0.39609498\n",
      "5900 0.39436787\n",
      "6000 0.39273146\n",
      "6100 0.39118016\n",
      "6200 0.38970867\n",
      "6300 0.38831228\n",
      "6400 0.386986\n",
      "6500 0.38572568\n",
      "6600 0.3845274\n",
      "6700 0.38338733\n",
      "6800 0.3823018\n",
      "6900 0.3812679\n",
      "7000 0.3802823\n",
      "7100 0.3793422\n",
      "7200 0.37844488\n",
      "7300 0.37758797\n",
      "7400 0.37676907\n",
      "7500 0.37598598\n",
      "7600 0.37523678\n",
      "7700 0.37451953\n",
      "7800 0.37383235\n",
      "7900 0.3731737\n",
      "8000 0.37254193\n",
      "8100 0.37193584\n",
      "8200 0.37135375\n",
      "8300 0.3707946\n",
      "8400 0.37025702\n",
      "8500 0.3697401\n",
      "8600 0.3692426\n",
      "8700 0.36876363\n",
      "8800 0.36830235\n",
      "8900 0.36785775\n",
      "9000 0.36742896\n",
      "9100 0.36701542\n",
      "9200 0.36661625\n",
      "9300 0.36623085\n",
      "9400 0.36585838\n",
      "9500 0.3654986\n",
      "9600 0.36515075\n",
      "9700 0.36481407\n",
      "9800 0.36448848\n",
      "9900 0.36417308\n",
      "10000 0.36386773\n",
      "\n",
      "Hypothesis:  [[0.0150041  0.01426423]\n",
      " [0.5014875  0.5017551 ]\n",
      " [0.96531683 0.965299  ]\n",
      " [0.5108993  0.511204  ]] \n",
      "Predicted:  [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "#멀티레이어 퍼셉트론 기반 신경망\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "#히든 레이어\n",
    "w1 = tf.Variable(tf.random_normal([2, 2]))\n",
    "b1 = tf.Variable(tf.random_normal([2]))\n",
    "layer1=tf.sigmoid(tf.matmul(x, w1) + b1)\n",
    "\n",
    "#출력 레이어\n",
    "w2 = tf.Variable(tf.random_normal([2, 2]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "hf=tf.sigmoid(tf.matmul(layer1, w2) + b2)\n",
    "\n",
    "# #출력 레이어\n",
    "# w3 = tf.Variable(tf.random_normal([2, 2]))\n",
    "# b3 = tf.Variable(tf.random_normal([2]))\n",
    "# layer3=tf.sigmoid(tf.matmul(layer2, w3) + b3)\n",
    "\n",
    "# #출력 레이어\n",
    "# w4 = tf.Variable(tf.random_normal([2, 1]))\n",
    "# b4 = tf.Variable(tf.random_normal([1]))\n",
    "# hf=tf.sigmoid(tf.matmul(layer3, w4) + b4)\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hf) + (1 - y) * tf.log(1 - hf))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hf > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cv = sess.run([train, cost], feed_dict={x: xdata, y: ydata})\n",
    "        if step % 100 == 0:\n",
    "            print(step, cv)\n",
    "\n",
    "    h, p, a = sess.run([hf, predicted, accuracy], feed_dict={x: xdata, y: ydata})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nPredicted: \", p, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hf:  [[1.9140578e-05]\n",
      " [9.9996948e-01]\n",
      " [9.9998140e-01]\n",
      " [2.6287278e-05]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# wide & deep\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 10]))\n",
    "b1 = tf.Variable(tf.random_normal([10]))\n",
    "layer1= tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b2 = tf.Variable(tf.random_normal([10]))\n",
    "layer2= tf.nn.relu(tf.matmul(layer1, w2) + b2)\n",
    "\n",
    "w3 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "layer3= tf.nn.relu(tf.matmul(layer2, w3) + b3)\n",
    "\n",
    "w4 = tf.Variable(tf.random_normal([10,1]))\n",
    "b4 = tf.Variable(tf.random_normal([1]))\n",
    "hf= tf.sigmoid(tf.matmul(layer3, w4) + b4)\n",
    "#tf.nn.relu\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hf) + (1 - y) * tf.log(1 - hf))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hf > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cv, wv = sess.run(\n",
    "                  [train, cost, w], feed_dict={x: xdata, y: ydata}\n",
    "        )\n",
    "#         if step % 100 == 0:\n",
    "#             print(step, cv, wv)\n",
    "\n",
    "    h, p, a = sess.run([hf, predicted, accuracy], feed_dict={x: xdata, y: ydata})\n",
    "    print(\"\\nhf: \",h, \"\\nCorrect: \", p, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.io\n",
    "# Dense검색\n",
    "#Dense(8, input_dim=4, init='uniform', activation='relu')\n",
    "#Dense(1, input_dim=3, activation='sigmoid')\n",
    "#Dense(10,input_dim=4, activation='softmax')\n",
    "#...\n",
    "\n",
    "# model=Sequential()\n",
    "# model.add(Dense(8, input_dim=4, init='uniform', activation='relu'))\n",
    "# model.add(Dense(1, input_dim=3, activation='sigmoid'))\n",
    "# model.add(Dense(10,input_dim=4, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.mnist - 90% 정확도 넘게...(텐서플로우)\n",
    "#2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
